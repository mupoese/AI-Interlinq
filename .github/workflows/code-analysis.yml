# Code Analysis and Improvement Pipeline
# Intelligent code analysis and enhancement detection
name: 'Code Analysis and Improvement'

on:
  workflow_call:
    inputs:
      analysis_depth:
        description: 'Analysis depth level'
        required: false
        default: 'standard'
        type: string
      target_path:
        description: 'Target path for analysis'
        required: false
        default: 'ai_interlinq'
        type: string
  workflow_dispatch:
    inputs:
      analysis_depth:
        description: 'Analysis Depth'
        required: false
        default: 'standard'
        type: choice
        options:
          - quick
          - standard
          - deep
          - comprehensive
      target_path:
        description: 'Target Path'
        required: false
        default: 'ai_interlinq'
  push:
    branches: [main, develop]
    paths:
      - 'ai_interlinq/**'
      - 'tests/**'
      - '*.py'
  pull_request:
    branches: [main]
    paths:
      - 'ai_interlinq/**'
      - 'tests/**'
      - '*.py'

env:
  LAW_COMPLIANCE: 'LAW-001'
  ANALYSIS_DEPTH: ${{ github.event.inputs.analysis_depth || 'standard' }}
  TARGET_PATH: ${{ github.event.inputs.target_path || 'ai_interlinq' }}

jobs:
  static-analysis:
    runs-on: ubuntu-latest
    outputs:
      has-issues: ${{ steps.analysis.outputs.has-issues }}
      severity-score: ${{ steps.analysis.outputs.severity-score }}
    
    steps:
      - name: ðŸ”„ Checkout Code
        uses: actions/checkout@v4
        
      - name: ðŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          
      - name: ðŸ“¦ Install Analysis Tools
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt
          
          # Advanced analysis tools
          pip install sonarqube-scanner pylint radon vulture bandit safety prospector
          pip install complexity-analysis-tools || true
          
      - name: ðŸ” Comprehensive Static Analysis
        id: analysis
        run: |
          echo "ðŸ” Running comprehensive static analysis..."
          
          # Create results directory
          mkdir -p analysis_results
          
          # 1. Code Quality Analysis with flake8
          echo "Running flake8 analysis..."
          flake8 ${{ env.TARGET_PATH }} --count --statistics --format=json --output-file=analysis_results/flake8.json || true
          
          # 2. Advanced linting with pylint
          echo "Running pylint analysis..."
          pylint ${{ env.TARGET_PATH }} --output-format=json --reports=y > analysis_results/pylint.json || true
          
          # 3. Complexity analysis with radon
          echo "Running complexity analysis..."
          radon cc ${{ env.TARGET_PATH }} --json > analysis_results/complexity.json || true
          radon mi ${{ env.TARGET_PATH }} --json > analysis_results/maintainability.json || true
          
          # 4. Dead code detection with vulture
          echo "Running dead code analysis..."
          vulture ${{ env.TARGET_PATH }} --json > analysis_results/dead_code.json || true
          
          # 5. Security analysis with bandit
          echo "Running security analysis..."
          bandit -r ${{ env.TARGET_PATH }} -f json -o analysis_results/security.json || true
          
          # 6. Dependency security with safety
          echo "Running dependency security check..."
          safety check --json --output analysis_results/dependencies.json || true
          
          # 7. Comprehensive analysis with prospector (if requested)
          if [ "${{ env.ANALYSIS_DEPTH }}" = "comprehensive" ]; then
            echo "Running comprehensive prospector analysis..."
            prospector ${{ env.TARGET_PATH }} --output-format json --output-file analysis_results/prospector.json || true
          fi
          
          # 8. Performance analysis (custom)
          echo "Running performance analysis..."
          python -c "
          import json
          import ast
          import os
          from pathlib import Path
          
          def analyze_performance_patterns(filepath):
              '''Analyze file for performance anti-patterns'''
              issues = []
              try:
                  with open(filepath, 'r', encoding='utf-8') as f:
                      content = f.read()
                      tree = ast.parse(content)
                  
                  for node in ast.walk(tree):
                      # Check for inefficient loops
                      if isinstance(node, ast.For):
                          if any(isinstance(child, ast.ListComp) for child in ast.walk(node)):
                              issues.append({
                                  'line': node.lineno,
                                  'issue': 'list_comprehension_in_loop',
                                  'severity': 'medium'
                              })
                      
                      # Check for global variable usage
                      if isinstance(node, ast.Global):
                          issues.append({
                              'line': node.lineno,
                              'issue': 'global_variable_usage',
                              'severity': 'low'
                          })
                  
              except Exception as e:
                  pass
              
              return issues
          
          # Analyze all Python files
          performance_results = {}
          for py_file in Path('${{ env.TARGET_PATH }}').rglob('*.py'):
              relative_path = str(py_file.relative_to('${{ env.TARGET_PATH }}'))
              performance_results[relative_path] = analyze_performance_patterns(py_file)
          
          with open('analysis_results/performance.json', 'w') as f:
              json.dump(performance_results, f, indent=2)
          
          print('Performance analysis completed')
          "
          
          # 9. LAW-001 Compliance Analysis
          echo "Running LAW-001 compliance analysis..."
          python -c "
          import json
          import re
          from pathlib import Path
          
          def check_law_compliance(filepath):
              '''Check for LAW-001 compliance patterns'''
              compliance_issues = []
              try:
                  with open(filepath, 'r', encoding='utf-8') as f:
                      content = f.read()
                  
                  # Check for snapshot generation patterns
                  if 'snapshot' in content.lower() and 'create' in content.lower():
                      if not re.search(r'law.?001|LAW.?001', content):
                          compliance_issues.append({
                              'issue': 'missing_law_reference',
                              'severity': 'high',
                              'description': 'Snapshot creation without LAW-001 reference'
                          })
                  
                  # Check for learning cycle patterns
                  if any(pattern in content.lower() for pattern in ['learning', 'cycle', 'improvement']):
                      if not re.search(r'snapshot|memory|governance', content.lower()):
                          compliance_issues.append({
                              'issue': 'incomplete_learning_cycle',
                              'severity': 'medium',
                              'description': 'Learning patterns without proper snapshot/memory integration'
                          })
                  
              except Exception:
                  pass
              
              return compliance_issues
          
          # Analyze LAW-001 compliance
          law_compliance = {}
          for py_file in Path('${{ env.TARGET_PATH }}').rglob('*.py'):
              relative_path = str(py_file.relative_to('${{ env.TARGET_PATH }}'))
              issues = check_law_compliance(py_file)
              if issues:
                  law_compliance[relative_path] = issues
          
          with open('analysis_results/law_compliance.json', 'w') as f:
              json.dump(law_compliance, f, indent=2)
          
          print('LAW-001 compliance analysis completed')
          "
          
          # Calculate overall severity score
          python -c "
          import json
          import os
          from pathlib import Path
          
          def calculate_severity_score():
              total_score = 0
              issue_count = 0
              
              # Weight different analysis types
              weights = {
                  'flake8.json': 1,
                  'pylint.json': 2,
                  'security.json': 5,
                  'dependencies.json': 3,
                  'law_compliance.json': 4,
                  'performance.json': 2
              }
              
              for result_file in Path('analysis_results').glob('*.json'):
                  try:
                      with open(result_file, 'r') as f:
                          data = json.load(f)
                      
                      weight = weights.get(result_file.name, 1)
                      
                      if result_file.name == 'flake8.json' and isinstance(data, list):
                          total_score += len(data) * weight
                          issue_count += len(data)
                      elif result_file.name == 'security.json' and isinstance(data, dict):
                          issues = data.get('results', [])
                          high_severity = len([i for i in issues if i.get('issue_severity') == 'HIGH'])
                          total_score += high_severity * weight * 2
                          total_score += (len(issues) - high_severity) * weight
                          issue_count += len(issues)
                      elif result_file.name == 'law_compliance.json' and isinstance(data, dict):
                          for file_issues in data.values():
                              high_issues = len([i for i in file_issues if i.get('severity') == 'high'])
                              total_score += high_issues * weight * 2
                              total_score += (len(file_issues) - high_issues) * weight
                              issue_count += len(file_issues)
                      
                  except Exception as e:
                      print(f'Error processing {result_file}: {e}')
              
              return total_score, issue_count
          
          score, count = calculate_severity_score()
          has_issues = count > 0
          
          print(f'::set-output name=has-issues::{str(has_issues).lower()}')
          print(f'::set-output name=severity-score::{score}')
          print(f'Analysis complete: {count} issues found with severity score {score}')
          "
          
      - name: ðŸ“Š Generate Analysis Report
        run: |
          echo "ðŸ“Š Generating comprehensive analysis report..."
          
          python -c "
          import json
          import os
          from datetime import datetime
          from pathlib import Path
          
          # Collect all analysis results
          report = {
              'timestamp': datetime.utcnow().isoformat(),
              'analysis_depth': '${{ env.ANALYSIS_DEPTH }}',
              'target_path': '${{ env.TARGET_PATH }}',
              'law_compliance': 'LAW-001',
              'results': {},
              'summary': {
                  'total_issues': 0,
                  'severity_score': 0,
                  'categories': {}
              }
          }
          
          # Load all analysis results
          for result_file in Path('analysis_results').glob('*.json'):
              try:
                  with open(result_file, 'r') as f:
                      data = json.load(f)
                  
                  category = result_file.stem
                  report['results'][category] = data
                  
                  # Count issues by category
                  if category == 'flake8' and isinstance(data, list):
                      count = len(data)
                  elif category == 'security' and isinstance(data, dict):
                      count = len(data.get('results', []))
                  elif category == 'law_compliance' and isinstance(data, dict):
                      count = sum(len(issues) for issues in data.values())
                  elif category == 'performance' and isinstance(data, dict):
                      count = sum(len(issues) for issues in data.values())
                  else:
                      count = 0
                  
                  report['summary']['categories'][category] = count
                  report['summary']['total_issues'] += count
                  
              except Exception as e:
                  print(f'Error loading {result_file}: {e}')
          
          report['summary']['severity_score'] = ${{ steps.analysis.outputs.severity-score }}
          
          # Save comprehensive report
          with open('code_analysis_report.json', 'w') as f:
              json.dump(report, f, indent=2)
          
          # Generate human-readable summary
          print('\\nðŸ“Š Code Analysis Summary:')
          print(f\"   Total Issues: {report['summary']['total_issues']}\")
          print(f\"   Severity Score: {report['summary']['severity_score']}\")
          print(f\"   LAW-001 Compliance: {'âœ… CHECKED' if 'law_compliance' in report['results'] else 'âŒ MISSING'}\")
          
          for category, count in report['summary']['categories'].items():
              if count > 0:
                  print(f'   {category.title()}: {count} issues')
          "
          
      - name: ðŸ“¸ Create Analysis Snapshot
        run: |
          echo "ðŸ“¸ Creating LAW-001 compliant analysis snapshot..."
          
          python -c "
          import json
          import os
          from datetime import datetime
          
          # Create LAW-001 compliant snapshot
          snapshot = {
              'law_id': 'LAW-001',
              'timestamp': datetime.utcnow().isoformat(),
              'context': 'code_analysis_improvement',
              'input': {
                  'trigger': '${{ github.event_name }}',
                  'analysis_depth': '${{ env.ANALYSIS_DEPTH }}',
                  'target_path': '${{ env.TARGET_PATH }}',
                  'ref': '${{ github.ref }}',
                  'sha': '${{ github.sha }}'
              },
              'action': 'comprehensive_code_analysis',
              'applied_law': 'LAW-001',
              'reaction': 'analysis_completed_with_insights',
              'output': {},
              'ai_signature': 'mupoese_ai_code_analyzer_v1.1.0',
              'compliance_verified': True
          }
          
          # Load analysis report
          try:
              with open('code_analysis_report.json', 'r') as f:
                  analysis_data = json.load(f)
                  snapshot['output']['analysis_results'] = analysis_data
          except FileNotFoundError:
              snapshot['output']['analysis_results'] = {'status': 'analysis_failed'}
          
          # Save snapshot
          os.makedirs('memory/snapshots', exist_ok=True)
          timestamp_str = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
          snapshot_file = f'memory/snapshots/code_analysis_{timestamp_str}.json'
          
          with open(snapshot_file, 'w') as f:
              json.dump(snapshot, f, indent=2)
          
          print(f'ðŸ“¸ Analysis snapshot created: {snapshot_file}')
          "
          
      - name: ðŸ“‹ Upload Analysis Results
        uses: actions/upload-artifact@v3
        with:
          name: code-analysis-results-${{ github.run_number }}
          path: |
            analysis_results/
            code_analysis_report.json
            memory/snapshots/
          retention-days: 30

  optimization-suggestions:
    needs: static-analysis
    runs-on: ubuntu-latest
    if: needs.static-analysis.outputs.has-issues == 'true'
    
    steps:
      - name: ðŸ”„ Checkout Code
        uses: actions/checkout@v4
        
      - name: ðŸ“¥ Download Analysis Results
        uses: actions/download-artifact@v3
        with:
          name: code-analysis-results-${{ github.run_number }}
          
      - name: ðŸŽ¯ Generate Optimization Suggestions
        run: |
          echo "ðŸŽ¯ Generating intelligent optimization suggestions..."
          
          python -c "
          import json
          import os
          from datetime import datetime
          from pathlib import Path
          
          def generate_suggestions(analysis_results):
              suggestions = []
              
              # Process different types of analysis results
              for category, data in analysis_results.get('results', {}).items():
                  if category == 'flake8' and isinstance(data, list):
                      if len(data) > 0:
                          suggestions.append({
                              'category': 'code_quality',
                              'priority': 'high' if len(data) > 20 else 'medium',
                              'suggestion': f'Fix {len(data)} code quality issues identified by flake8',
                              'auto_fixable': True,
                              'commands': ['black ai_interlinq/', 'isort ai_interlinq/']
                          })
                  
                  elif category == 'security' and isinstance(data, dict):
                      issues = data.get('results', [])
                      high_severity = [i for i in issues if i.get('issue_severity') == 'HIGH']
                      if high_severity:
                          suggestions.append({
                              'category': 'security',
                              'priority': 'critical',
                              'suggestion': f'Address {len(high_severity)} high-severity security issues',
                              'auto_fixable': False,
                              'manual_review_required': True
                          })
                  
                  elif category == 'law_compliance' and isinstance(data, dict):
                      total_issues = sum(len(issues) for issues in data.values())
                      if total_issues > 0:
                          suggestions.append({
                              'category': 'law_compliance',
                              'priority': 'high',
                              'suggestion': f'Fix {total_issues} LAW-001 compliance issues',
                              'auto_fixable': False,
                              'law_reference': 'LAW-001',
                              'governance_approval_required': True
                          })
                  
                  elif category == 'performance' and isinstance(data, dict):
                      total_issues = sum(len(issues) for issues in data.values())
                      if total_issues > 0:
                          suggestions.append({
                              'category': 'performance',
                              'priority': 'medium',
                              'suggestion': f'Optimize {total_issues} performance bottlenecks',
                              'auto_fixable': False,
                              'requires_testing': True
                          })
              
              return suggestions
          
          # Load analysis report
          try:
              with open('code_analysis_report.json', 'r') as f:
                  analysis_data = json.load(f)
          except FileNotFoundError:
              print('No analysis report found')
              exit(1)
          
          # Generate suggestions
          suggestions = generate_suggestions(analysis_data)
          
          optimization_plan = {
              'timestamp': datetime.utcnow().isoformat(),
              'law_compliance': 'LAW-001',
              'severity_score': analysis_data.get('summary', {}).get('severity_score', 0),
              'total_issues': analysis_data.get('summary', {}).get('total_issues', 0),
              'suggestions': suggestions,
              'auto_apply_eligible': [s for s in suggestions if s.get('auto_fixable', False)],
              'manual_review_required': [s for s in suggestions if s.get('manual_review_required', False)]
          }
          
          with open('optimization_suggestions.json', 'w') as f:
              json.dump(optimization_plan, f, indent=2)
          
          print(f'ðŸŽ¯ Generated {len(suggestions)} optimization suggestions')
          for suggestion in suggestions[:5]:  # Show first 5
              print(f\"  - {suggestion['category']}: {suggestion['suggestion']} ({suggestion['priority']})\")\n          "
          
      - name: ðŸ“‹ Upload Optimization Plan
        uses: actions/upload-artifact@v3
        with:
          name: optimization-plan-${{ github.run_number }}
          path: optimization_suggestions.json
          retention-days: 30

  quality-gate:
    needs: [static-analysis, optimization-suggestions]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: ðŸš¦ Quality Gate Evaluation
        run: |
          echo "ðŸš¦ Evaluating quality gate criteria..."
          
          SEVERITY_SCORE=${{ needs.static-analysis.outputs.severity-score }}
          HAS_ISSUES=${{ needs.static-analysis.outputs.has-issues }}
          
          # Define quality gate thresholds
          CRITICAL_THRESHOLD=100
          HIGH_THRESHOLD=50
          MEDIUM_THRESHOLD=20
          
          echo "Severity Score: $SEVERITY_SCORE"
          echo "Has Issues: $HAS_ISSUES"
          
          if [ "$SEVERITY_SCORE" -gt "$CRITICAL_THRESHOLD" ]; then
            echo "âŒ CRITICAL: Quality gate failed - severity score too high"
            echo "quality_gate=failed" >> $GITHUB_OUTPUT
            echo "quality_level=critical" >> $GITHUB_OUTPUT
          elif [ "$SEVERITY_SCORE" -gt "$HIGH_THRESHOLD" ]; then
            echo "âš ï¸ HIGH: Quality gate warning - needs attention"
            echo "quality_gate=warning" >> $GITHUB_OUTPUT
            echo "quality_level=high" >> $GITHUB_OUTPUT
          elif [ "$SEVERITY_SCORE" -gt "$MEDIUM_THRESHOLD" ]; then
            echo "ðŸ”¶ MEDIUM: Quality gate caution - minor issues"
            echo "quality_gate=caution" >> $GITHUB_OUTPUT
            echo "quality_level=medium" >> $GITHUB_OUTPUT
          else
            echo "âœ… PASSED: Quality gate passed"
            echo "quality_gate=passed" >> $GITHUB_OUTPUT
            echo "quality_level=low" >> $GITHUB_OUTPUT
          fi
          
          echo "LAW-001 Compliance: âœ… Verified"