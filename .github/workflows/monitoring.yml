# Monitoring and Alerting System
# Continuous system monitoring and alert management
name: 'Monitoring and Alerting'

on:
  schedule:
    - cron: '*/15 * * * *'  # Every 15 minutes
    - cron: '0 * * * *'     # Hourly detailed check
    - cron: '0 0 * * *'     # Daily comprehensive report
  workflow_dispatch:
    inputs:
      monitoring_level:
        description: 'Monitoring Level'
        required: false
        default: 'standard'
        type: choice
        options:
          - quick
          - standard
          - comprehensive
          - emergency
      alert_threshold:
        description: 'Alert Threshold'
        required: false
        default: 75
        type: number
  workflow_call:
    inputs:
      monitoring_level:
        description: 'Monitoring level to execute'
        required: false
        default: 'standard'
        type: string

env:
  LAW_COMPLIANCE: 'LAW-001'
  MONITORING_LEVEL: ${{ github.event.inputs.monitoring_level || 'standard' }}
  ALERT_THRESHOLD: ${{ github.event.inputs.alert_threshold || 75 }}

jobs:
  system-health-monitoring:
    runs-on: ubuntu-latest
    outputs:
      health-score: ${{ steps.health.outputs.score }}
      alert-level: ${{ steps.health.outputs.alert_level }}
      requires-attention: ${{ steps.health.outputs.requires_attention }}
    
    steps:
      - name: 🔄 Checkout Repository
        uses: actions/checkout@v4
        
      - name: 🐍 Setup Python Environment
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          
      - name: 📦 Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          pip install psutil requests
          
      - name: 🏥 System Health Assessment
        id: health
        run: |
          echo "🏥 Performing comprehensive system health assessment..."
          
          python -c "
          import json
          import os
          import sys
          import subprocess
          import psutil
          import time
          from datetime import datetime, timedelta
          from pathlib import Path
          
          def assess_system_health():
              health_data = {
                  'timestamp': datetime.utcnow().isoformat(),
                  'monitoring_level': '${{ env.MONITORING_LEVEL }}',
                  'law_compliance': 'LAW-001',
                  'checks': {},
                  'metrics': {},
                  'alerts': [],
                  'overall_score': 0
              }
              
              # 1. Repository Structure Health
              print('Checking repository structure...')
              structure_score = 100
              
              required_dirs = ['ai_interlinq', 'tests', '.github/workflows', 'memory', 'governance']
              for req_dir in required_dirs:
                  if Path(req_dir).exists():
                      health_data['checks'][f'{req_dir}_exists'] = True
                  else:
                      health_data['checks'][f'{req_dir}_exists'] = False
                      structure_score -= 15
                      health_data['alerts'].append({
                          'type': 'missing_directory',
                          'severity': 'medium',
                          'message': f'Required directory {req_dir} is missing'
                      })
              
              health_data['metrics']['structure_score'] = structure_score
              
              # 2. Code Quality Health
              print('Assessing code quality...')
              quality_score = 100
              
              try:
                  # Check for syntax errors
                  result = subprocess.run([
                      'python', '-m', 'py_compile', 'ai_interlinq/__init__.py'
                  ], capture_output=True)
                  
                  if result.returncode == 0:
                      health_data['checks']['syntax_valid'] = True
                  else:
                      health_data['checks']['syntax_valid'] = False
                      quality_score -= 30
                      health_data['alerts'].append({
                          'type': 'syntax_error',
                          'severity': 'high',
                          'message': 'Syntax errors detected in main package'
                      })
              except Exception:
                  quality_score -= 20
              
              # Check import health
              try:
                  import ai_interlinq
                  health_data['checks']['import_successful'] = True
              except ImportError as e:
                  health_data['checks']['import_successful'] = False
                  quality_score -= 40
                  health_data['alerts'].append({
                      'type': 'import_error',
                      'severity': 'critical',
                      'message': f'Package import failed: {str(e)}'
                  })
              
              health_data['metrics']['quality_score'] = quality_score
              
              # 3. LAW-001 Compliance Health
              print('Verifying LAW-001 compliance...')
              compliance_score = 100
              
              # Check law.ai file
              if Path('law.ai').exists():
                  with open('law.ai', 'r') as f:
                      law_content = f.read()
                  
                  if 'LAW-001' in law_content:
                      health_data['checks']['law_file_valid'] = True
                  else:
                      health_data['checks']['law_file_valid'] = False
                      compliance_score -= 25
                      health_data['alerts'].append({
                          'type': 'law_compliance',
                          'severity': 'high',
                          'message': 'LAW-001 reference missing from law.ai file'
                      })
              else:
                  health_data['checks']['law_file_valid'] = False
                  compliance_score -= 50
                  health_data['alerts'].append({
                      'type': 'law_compliance',
                      'severity': 'critical',
                      'message': 'LAW-001 law.ai file is missing'
                  })
              
              # Check core learning modules
              core_modules = [
                  'ai_interlinq/core/learning_cycle.py',
                  'ai_interlinq/core/snapshot_manager.py',
                  'ai_interlinq/core/pattern_detector.py'
              ]
              
              missing_modules = []
              for module in core_modules:
                  if not Path(module).exists():
                      missing_modules.append(module)
                      compliance_score -= 10
              
              if missing_modules:
                  health_data['alerts'].append({
                      'type': 'missing_core_modules',
                      'severity': 'medium',
                      'message': f'Missing core modules: {missing_modules}'
                  })
              
              health_data['metrics']['compliance_score'] = compliance_score
              
              # 4. Performance Health
              print('Checking performance metrics...')
              performance_score = 100
              
              # Memory usage
              process = psutil.Process()
              memory_mb = process.memory_info().rss / 1024 / 1024
              health_data['metrics']['memory_usage_mb'] = round(memory_mb, 2)
              
              if memory_mb > 500:  # Alert if over 500MB
                  performance_score -= 20
                  health_data['alerts'].append({
                      'type': 'memory_usage',
                      'severity': 'medium',
                      'message': f'High memory usage: {memory_mb:.1f}MB'
                  })
              
              # CPU usage
              cpu_percent = psutil.cpu_percent(interval=1)
              health_data['metrics']['cpu_usage_percent'] = cpu_percent
              
              if cpu_percent > 80:
                  performance_score -= 15
                  health_data['alerts'].append({
                      'type': 'cpu_usage',
                      'severity': 'medium',
                      'message': f'High CPU usage: {cpu_percent:.1f}%'
                  })
              
              health_data['metrics']['performance_score'] = performance_score
              
              # 5. Security Health
              print('Performing security assessment...')
              security_score = 100
              
              # Check for common security files
              security_files = ['.gitignore', 'requirements.txt']
              for sec_file in security_files:
                  if not Path(sec_file).exists():
                      security_score -= 10
                      health_data['alerts'].append({
                          'type': 'security_file_missing',
                          'severity': 'low',
                          'message': f'Security-related file {sec_file} is missing'
                      })
              
              # Check for secrets in code (basic check)
              sensitive_patterns = ['password', 'api_key', 'secret_key', 'token']
              suspicious_files = []
              
              for py_file in Path('ai_interlinq').rglob('*.py'):
                  try:
                      with open(py_file, 'r', encoding='utf-8') as f:
                          content = f.read().lower()
                      
                      for pattern in sensitive_patterns:
                          if pattern in content and '=' in content:
                              suspicious_files.append(str(py_file))
                              break
                  except Exception:
                      pass
              
              if suspicious_files:
                  security_score -= 30
                  health_data['alerts'].append({
                      'type': 'potential_secrets',
                      'severity': 'high',
                      'message': f'Potential secrets found in: {suspicious_files[:3]}'
                  })
              
              health_data['metrics']['security_score'] = security_score
              
              # 6. Snapshot System Health
              print('Checking snapshot system health...')
              snapshot_score = 100
              
              if Path('memory/snapshots').exists():
                  snapshot_files = list(Path('memory/snapshots').glob('*.json'))
                  health_data['metrics']['snapshot_count'] = len(snapshot_files)
                  
                  # Check for recent snapshots
                  recent_snapshots = 0
                  cutoff_time = datetime.utcnow() - timedelta(hours=24)
                  
                  for snapshot_file in snapshot_files:
                      try:
                          mtime = datetime.fromtimestamp(snapshot_file.stat().st_mtime)
                          if mtime > cutoff_time:
                              recent_snapshots += 1
                      except Exception:
                          pass
                  
                  health_data['metrics']['recent_snapshots'] = recent_snapshots
                  
                  if recent_snapshots == 0:
                      snapshot_score -= 25
                      health_data['alerts'].append({
                          'type': 'stale_snapshots',
                          'severity': 'medium',
                          'message': 'No recent snapshots found (24h)'
                      })
              else:
                  snapshot_score -= 50
                  health_data['alerts'].append({
                      'type': 'snapshot_system',
                      'severity': 'high',
                      'message': 'Snapshot directory not found'
                  })
              
              health_data['metrics']['snapshot_score'] = snapshot_score
              
              # Calculate overall health score
              scores = [
                  health_data['metrics']['structure_score'],
                  health_data['metrics']['quality_score'],
                  health_data['metrics']['compliance_score'],
                  health_data['metrics']['performance_score'],
                  health_data['metrics']['security_score'],
                  health_data['metrics']['snapshot_score']
              ]
              
              health_data['overall_score'] = round(sum(scores) / len(scores), 1)
              
              # Determine alert level
              if health_data['overall_score'] >= 90:
                  alert_level = 'green'
              elif health_data['overall_score'] >= 75:
                  alert_level = 'yellow'
              elif health_data['overall_score'] >= 50:
                  alert_level = 'orange'
              else:
                  alert_level = 'red'
              
              health_data['alert_level'] = alert_level
              health_data['requires_attention'] = alert_level in ['orange', 'red']
              
              return health_data
          
          # Perform health assessment
          health_results = assess_system_health()
          
          # Save results
          with open('system_health_report.json', 'w') as f:
              json.dump(health_results, f, indent=2)
          
          # Output for GitHub Actions
          print(f'::set-output name=score::{health_results[\"overall_score\"]}')
          print(f'::set-output name=alert_level::{health_results[\"alert_level\"]}')
          print(f'::set-output name=requires_attention::{str(health_results[\"requires_attention\"]).lower()}')
          
          print(f'\\n🏥 System Health Report:')
          print(f'   Overall Score: {health_results[\"overall_score\"]}/100')
          print(f'   Alert Level: {health_results[\"alert_level\"].upper()}')
          print(f'   Active Alerts: {len(health_results[\"alerts\"])}')
          
          if health_results['alerts']:
              print('\\n🚨 Active Alerts:')
              for alert in health_results['alerts'][:5]:  # Show first 5
                  severity_icon = {'low': '🔵', 'medium': '🟡', 'high': '🟠', 'critical': '🔴'}.get(alert['severity'], '⚪')
                  print(f'   {severity_icon} {alert[\"type\"]}: {alert[\"message\"]}')
          "
          
      - name: 📊 Performance Metrics Collection
        run: |
          echo "📊 Collecting detailed performance metrics..."
          
          python -c "
          import json
          import time
          import psutil
          import subprocess
          from datetime import datetime
          from pathlib import Path
          
          def collect_performance_metrics():
              metrics = {
                  'timestamp': datetime.utcnow().isoformat(),
                  'system': {},
                  'repository': {},
                  'workflows': {}
              }
              
              # System metrics
              metrics['system'] = {
                  'cpu_count': psutil.cpu_count(),
                  'cpu_percent': psutil.cpu_percent(interval=1),
                  'memory_total_gb': round(psutil.virtual_memory().total / (1024**3), 2),
                  'memory_available_gb': round(psutil.virtual_memory().available / (1024**3), 2),
                  'memory_percent': psutil.virtual_memory().percent,
                  'disk_usage_percent': psutil.disk_usage('.').percent
              }
              
              # Repository metrics
              try:
                  # Count files by type
                  py_files = len(list(Path('.').rglob('*.py')))
                  yml_files = len(list(Path('.').rglob('*.yml'))) + len(list(Path('.').rglob('*.yaml')))
                  test_files = len(list(Path('tests').rglob('*.py'))) if Path('tests').exists() else 0
                  
                  metrics['repository'] = {
                      'python_files': py_files,
                      'workflow_files': yml_files,
                      'test_files': test_files,
                      'total_size_mb': round(sum(f.stat().st_size for f in Path('.').rglob('*') if f.is_file()) / (1024*1024), 2)
                  }
                  
              except Exception as e:
                  metrics['repository'] = {'error': str(e)}
              
              # Workflow performance (if available)
              try:
                  # Check for recent workflow runs (simulate)
                  workflow_dir = Path('.github/workflows')
                  if workflow_dir.exists():
                      workflow_files = list(workflow_dir.glob('*.yml'))
                      metrics['workflows'] = {
                          'total_workflows': len(workflow_files),
                          'workflow_names': [f.stem for f in workflow_files]
                      }
              except Exception:
                  metrics['workflows'] = {'error': 'Unable to collect workflow metrics'}
              
              return metrics
          
          # Collect metrics
          performance_metrics = collect_performance_metrics()
          
          with open('performance_metrics.json', 'w') as f:
              json.dump(performance_metrics, f, indent=2)
          
          print('📊 Performance Metrics Summary:')
          print(f'   CPU Usage: {performance_metrics[\"system\"][\"cpu_percent\"]}%')
          print(f'   Memory Usage: {performance_metrics[\"system\"][\"memory_percent\"]}%')
          print(f'   Python Files: {performance_metrics[\"repository\"].get(\"python_files\", \"N/A\")}')
          print(f'   Repository Size: {performance_metrics[\"repository\"].get(\"total_size_mb\", \"N/A\")}MB')
          "
          
      - name: 📸 Create Monitoring Snapshot
        run: |
          echo "📸 Creating LAW-001 compliant monitoring snapshot..."
          
          python -c "
          import json
          import os
          from datetime import datetime
          
          # Load health and performance data
          health_data = {}
          performance_data = {}
          
          try:
              with open('system_health_report.json', 'r') as f:
                  health_data = json.load(f)
          except FileNotFoundError:
              pass
          
          try:
              with open('performance_metrics.json', 'r') as f:
                  performance_data = json.load(f)
          except FileNotFoundError:
              pass
          
          # Create LAW-001 compliant snapshot
          snapshot = {
              'law_id': 'LAW-001',
              'timestamp': datetime.utcnow().isoformat(),
              'context': 'system_monitoring_and_alerting',
              'input': {
                  'trigger': '${{ github.event_name }}',
                  'monitoring_level': '${{ env.MONITORING_LEVEL }}',
                  'alert_threshold': ${{ env.ALERT_THRESHOLD }}
              },
              'action': 'comprehensive_system_health_monitoring',
              'applied_law': 'LAW-001',
              'reaction': 'monitoring_completed_with_metrics',
              'output': {
                  'health_assessment': health_data,
                  'performance_metrics': performance_data,
                  'monitoring_status': 'active',
                  'alerts_generated': len(health_data.get('alerts', []))
              },
              'ai_signature': 'mupoese_ai_monitor_v1.1.0',
              'compliance_verified': True
          }
          
          # Save snapshot
          os.makedirs('memory/snapshots', exist_ok=True)
          timestamp_str = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
          snapshot_file = f'memory/snapshots/monitoring_{timestamp_str}.json'
          
          with open(snapshot_file, 'w') as f:
              json.dump(snapshot, f, indent=2)
          
          print(f'📸 Monitoring snapshot created: {snapshot_file}')
          "

  alert-processing:
    needs: system-health-monitoring
    runs-on: ubuntu-latest
    if: needs.system-health-monitoring.outputs.requires-attention == 'true'
    
    steps:
      - name: 🔄 Checkout Repository
        uses: actions/checkout@v4
        
      - name: 📥 Download Health Report
        uses: actions/download-artifact@v3
        with:
          name: monitoring-results-${{ github.run_number }}
        continue-on-error: true
        
      - name: 🚨 Process Critical Alerts
        run: |
          echo "🚨 Processing critical system alerts..."
          echo "Health Score: ${{ needs.system-health-monitoring.outputs.health-score }}"
          echo "Alert Level: ${{ needs.system-health-monitoring.outputs.alert-level }}"
          
          python -c "
          import json
          from datetime import datetime
          
          # Load health report if available
          try:
              with open('system_health_report.json', 'r') as f:
                  health_data = json.load(f)
          except FileNotFoundError:
              health_data = {'alerts': [], 'overall_score': 0}
          
          alerts = health_data.get('alerts', [])
          critical_alerts = [a for a in alerts if a.get('severity') == 'critical']
          high_alerts = [a for a in alerts if a.get('severity') == 'high']
          
          print(f'🚨 Alert Processing Summary:')
          print(f'   Total Alerts: {len(alerts)}')
          print(f'   Critical: {len(critical_alerts)}')
          print(f'   High Priority: {len(high_alerts)}')
          
          # Process critical alerts
          if critical_alerts:
              print('\\n🔴 CRITICAL ALERTS:')
              for alert in critical_alerts:
                  print(f'   - {alert[\"type\"]}: {alert[\"message\"]}')
          
          # Process high priority alerts  
          if high_alerts:
              print('\\n🟠 HIGH PRIORITY ALERTS:')
              for alert in high_alerts:
                  print(f'   - {alert[\"type\"]}: {alert[\"message\"]}')
          
          # Generate alert response actions
          response_actions = []
          
          for alert in critical_alerts + high_alerts:
              if alert['type'] == 'import_error':
                  response_actions.append('Fix package import issues immediately')
              elif alert['type'] == 'law_compliance':
                  response_actions.append('Restore LAW-001 compliance files')
              elif alert['type'] == 'syntax_error':
                  response_actions.append('Fix syntax errors in core files')
              elif alert['type'] == 'potential_secrets':
                  response_actions.append('Review and secure potential secret leaks')
          
          if response_actions:
              print('\\n⚡ RECOMMENDED ACTIONS:')
              for action in response_actions:
                  print(f'   - {action}')
          
          # Create alert summary
          alert_summary = {
              'timestamp': datetime.utcnow().isoformat(),
              'health_score': ${{ needs.system-health-monitoring.outputs.health-score }},
              'alert_level': '${{ needs.system-health-monitoring.outputs.alert-level }}',
              'total_alerts': len(alerts),
              'critical_count': len(critical_alerts),
              'high_count': len(high_alerts),
              'response_actions': response_actions,
              'requires_immediate_attention': len(critical_alerts) > 0
          }
          
          with open('alert_summary.json', 'w') as f:
              json.dump(alert_summary, f, indent=2)
          "
          
      - name: 🔧 Auto-Remediation Attempt
        run: |
          echo "🔧 Attempting automatic remediation for known issues..."
          
          python -c "
          import json
          import subprocess
          import os
          from pathlib import Path
          
          remediation_log = []
          
          try:
              with open('system_health_report.json', 'r') as f:
                  health_data = json.load(f)
          except FileNotFoundError:
              health_data = {'alerts': []}
          
          for alert in health_data.get('alerts', []):
              alert_type = alert.get('type', '')
              
              if alert_type == 'missing_directory':
                  # Create missing directories
                  missing_dir = alert['message'].split()[-1]  # Extract directory name
                  if missing_dir in ['memory', 'memory/snapshots']:
                      try:
                          os.makedirs(missing_dir, exist_ok=True)
                          remediation_log.append(f'Created missing directory: {missing_dir}')
                      except Exception as e:
                          remediation_log.append(f'Failed to create {missing_dir}: {e}')
              
              elif alert_type == 'law_compliance' and 'missing' in alert['message']:
                  # Attempt to restore basic LAW-001 file if completely missing
                  if not Path('law.ai').exists():
                      try:
                          basic_law_content = '''law.ai:
          ID: LAW-001
          Title: Cause-Input-Action-Law-Reaction-Output-Effect Learning Cycle
          Status: RESTORED_BY_MONITORING_SYSTEM
          
          Basic LAW-001 compliance restored by automated monitoring system.
          Manual review and proper restoration required.'''
                          
                          with open('law.ai', 'w') as f:
                              f.write(basic_law_content)
                          
                          remediation_log.append('Created basic LAW-001 file - requires manual review')
                      except Exception as e:
                          remediation_log.append(f'Failed to create law.ai: {e}')
              
              elif alert_type == 'security_file_missing':
                  # Create basic .gitignore if missing
                  if not Path('.gitignore').exists():
                      try:
                          basic_gitignore = '''__pycache__/
          *.pyc
          *.pyo
          *.pyd
          .Python
          env/
          venv/
          .venv/
          .env
          *.log
          .DS_Store'''
                          
                          with open('.gitignore', 'w') as f:
                              f.write(basic_gitignore)
                          
                          remediation_log.append('Created basic .gitignore file')
                      except Exception as e:
                          remediation_log.append(f'Failed to create .gitignore: {e}')
          
          print('🔧 Auto-Remediation Results:')
          if remediation_log:
              for log_entry in remediation_log:
                  print(f'   ✅ {log_entry}')
          else:
              print('   ℹ️ No automatic remediation available for current alerts')
          
          # Save remediation log
          with open('remediation_log.json', 'w') as f:
              json.dump({
                  'timestamp': '$(date -u)',
                  'actions_taken': remediation_log,
                  'auto_remediation_enabled': True
              }, f, indent=2)
          "

  comprehensive-reporting:
    needs: [system-health-monitoring, alert-processing]
    runs-on: ubuntu-latest
    if: always() && github.event.schedule == '0 0 * * *'  # Daily comprehensive report
    
    steps:
      - name: 🔄 Checkout Repository
        uses: actions/checkout@v4
        
      - name: 📊 Generate Daily Report
        run: |
          echo "📊 Generating comprehensive daily monitoring report..."
          
          python -c "
          import json
          import os
          from datetime import datetime, timedelta
          from pathlib import Path
          
          def generate_daily_report():
              report = {
                  'report_date': datetime.utcnow().date().isoformat(),
                  'report_type': 'daily_comprehensive',
                  'law_compliance': 'LAW-001',
                  'period': '24_hours',
                  'summary': {},
                  'trends': {},
                  'recommendations': []
              }
              
              # Load recent snapshots for trend analysis
              snapshot_dir = Path('memory/snapshots')
              recent_snapshots = []
              
              if snapshot_dir.exists():
                  cutoff = datetime.utcnow() - timedelta(days=1)
                  
                  for snapshot_file in snapshot_dir.glob('monitoring_*.json'):
                      try:
                          mtime = datetime.fromtimestamp(snapshot_file.stat().st_mtime)
                          if mtime > cutoff:
                              with open(snapshot_file, 'r') as f:
                                  snapshot_data = json.load(f)
                              recent_snapshots.append(snapshot_data)
                      except Exception:
                          continue
              
              # Analyze trends
              if recent_snapshots:
                  health_scores = []
                  alert_counts = []
                  
                  for snapshot in recent_snapshots:
                      output = snapshot.get('output', {})
                      health_data = output.get('health_assessment', {})
                      
                      if 'overall_score' in health_data:
                          health_scores.append(health_data['overall_score'])
                      
                      if 'alerts' in health_data:
                          alert_counts.append(len(health_data['alerts']))
                  
                  if health_scores:
                      report['trends']['health_score'] = {
                          'average': round(sum(health_scores) / len(health_scores), 1),
                          'min': min(health_scores),
                          'max': max(health_scores),
                          'samples': len(health_scores)
                      }
                  
                  if alert_counts:
                      report['trends']['alert_frequency'] = {
                          'average': round(sum(alert_counts) / len(alert_counts), 1),
                          'total': sum(alert_counts),
                          'max_in_period': max(alert_counts)
                      }
              
              # Generate recommendations
              if 'health_score' in report['trends']:
                  avg_score = report['trends']['health_score']['average']
                  
                  if avg_score < 60:
                      report['recommendations'].append({
                          'priority': 'critical',
                          'area': 'system_health',
                          'action': 'Immediate system review required - health score below 60%'
                      })
                  elif avg_score < 80:
                      report['recommendations'].append({
                          'priority': 'high',
                          'area': 'system_health', 
                          'action': 'System optimization recommended - health score below 80%'
                      })
              
              if 'alert_frequency' in report['trends']:
                  if report['trends']['alert_frequency']['average'] > 5:
                      report['recommendations'].append({
                          'priority': 'medium',
                          'area': 'alert_management',
                          'action': 'High alert frequency detected - review alert thresholds'
                      })
              
              # Summary
              report['summary'] = {
                  'monitoring_active': True,
                  'law_001_compliant': True,
                  'snapshots_analyzed': len(recent_snapshots),
                  'recommendations_count': len(report['recommendations']),
                  'overall_status': 'healthy' if report['trends'].get('health_score', {}).get('average', 0) >= 80 else 'needs_attention'
              }
              
              return report
          
          # Generate report
          daily_report = generate_daily_report()
          
          with open('daily_monitoring_report.json', 'w') as f:
              json.dump(daily_report, f, indent=2)
          
          print('📊 Daily Monitoring Report Generated:')
          print(f'   Report Date: {daily_report[\"report_date\"]}')
          print(f'   Overall Status: {daily_report[\"summary\"][\"overall_status\"].upper()}')
          print(f'   Snapshots Analyzed: {daily_report[\"summary\"][\"snapshots_analyzed\"]}')
          print(f'   Recommendations: {daily_report[\"summary\"][\"recommendations_count\"]}')
          
          if daily_report['trends']:
              print('\\n📈 Key Trends:')
              if 'health_score' in daily_report['trends']:
                  hs = daily_report['trends']['health_score']
                  print(f'   Health Score: {hs[\"average\"]}% avg (range: {hs[\"min\"]}-{hs[\"max\"]}%)')
              
              if 'alert_frequency' in daily_report['trends']:
                  af = daily_report['trends']['alert_frequency']
                  print(f'   Alert Frequency: {af[\"average\"]} avg, {af[\"total\"]} total')
          
          if daily_report['recommendations']:
              print('\\n💡 Recommendations:')
              for rec in daily_report['recommendations']:
                  priority_icon = {'critical': '🔴', 'high': '🟠', 'medium': '🟡', 'low': '🟢'}.get(rec['priority'], '⚪')
                  print(f'   {priority_icon} {rec[\"area\"]}: {rec[\"action\"]}')
          "

  upload-results:
    needs: [system-health-monitoring, alert-processing, comprehensive-reporting]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: 📥 Collect Monitoring Artifacts
        run: |
          echo "📥 Collecting all monitoring artifacts..."
          
          # Create consolidated results directory
          mkdir -p monitoring_results
          
          # Copy any generated files
          find . -name "*.json" -type f -exec cp {} monitoring_results/ \; 2>/dev/null || true
          find . -name "*_report.json" -type f -exec cp {} monitoring_results/ \; 2>/dev/null || true
          find . -name "*_log.json" -type f -exec cp {} monitoring_results/ \; 2>/dev/null || true
          
          # Create a summary file
          python -c "
          import json
          from datetime import datetime
          
          summary = {
              'timestamp': datetime.utcnow().isoformat(),
              'monitoring_run': '${{ github.run_number }}',
              'health_score': '${{ needs.system-health-monitoring.outputs.health-score }}',
              'alert_level': '${{ needs.system-health-monitoring.outputs.alert-level }}',
              'requires_attention': '${{ needs.system-health-monitoring.outputs.requires-attention }}',
              'law_compliance': 'LAW-001',
              'monitoring_level': '${{ env.MONITORING_LEVEL }}',
              'alert_threshold': ${{ env.ALERT_THRESHOLD }}
          }
          
          with open('monitoring_results/run_summary.json', 'w') as f:
              json.dump(summary, f, indent=2)
          
          print('📥 Monitoring artifacts collected')
          "
          
      - name: 📋 Upload Monitoring Results
        uses: actions/upload-artifact@v3
        with:
          name: monitoring-results-${{ github.run_number }}
          path: |
            monitoring_results/
            memory/snapshots/
          retention-days: 90

  notification:
    needs: [system-health-monitoring, alert-processing]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: 📬 Send Monitoring Notification
        run: |
          echo "📬 Monitoring and alerting cycle completed"
          echo "Health Score: ${{ needs.system-health-monitoring.outputs.health-score }}/100"
          echo "Alert Level: ${{ needs.system-health-monitoring.outputs.alert-level }}"
          echo "Requires Attention: ${{ needs.system-health-monitoring.outputs.requires-attention }}"
          echo "LAW-001 Compliance: ✅"
          echo "Monitoring Level: ${{ env.MONITORING_LEVEL }}"
          echo "Timestamp: $(date -u)"