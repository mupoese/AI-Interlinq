# Comprehensive Testing Pipeline
# Complete testing automation with quality gates and LAW-001 compliance
name: 'Comprehensive Testing Pipeline'

on:
  workflow_call:
    inputs:
      test_suite:
        description: 'Test suite to run'
        required: false
        default: 'all'
        type: string
      coverage_threshold:
        description: 'Coverage threshold percentage'
        required: false
        default: '80'
        type: string
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Test Suite'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - integration
          - performance
          - security
          - law_compliance
      coverage_threshold:
        description: 'Coverage Threshold (%)'
        required: false
        default: '80'
      parallel_execution:
        description: 'Enable Parallel Execution'
        required: false
        default: true
        type: boolean
  push:
    branches: [main, develop]
    paths:
      - 'ai_interlinq/**'
      - 'tests/**'
      - 'requirements*.txt'
  pull_request:
    branches: [main]

env:
  LAW_COMPLIANCE: 'LAW-001'
  TEST_SUITE: ${{ github.event.inputs.test_suite || 'all' }}
  COVERAGE_THRESHOLD: ${{ github.event.inputs.coverage_threshold || '80' }}
  PARALLEL_EXECUTION: ${{ github.event.inputs.parallel_execution || 'true' }}
  PYTHON_VERSION: '3.11'

jobs:
  test-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - name: 🔄 Set Test Matrix
        id: set-matrix
        run: |
          if [ "${{ env.PARALLEL_EXECUTION }}" = "true" ]; then
            echo 'matrix={"python-version": ["3.9", "3.10", "3.11"], "test-type": ["unit", "integration"]}' >> $GITHUB_OUTPUT
          else
            echo 'matrix={"python-version": ["3.11"], "test-type": ["all"]}' >> $GITHUB_OUTPUT
          fi

  unit-tests:
    runs-on: ubuntu-latest
    needs: test-matrix
    if: contains(github.event.inputs.test_suite, 'unit') || contains(github.event.inputs.test_suite, 'all')
    strategy:
      matrix: ${{ fromJson(needs.test-matrix.outputs.matrix) }}
      fail-fast: false
    
    steps:
      - name: 🔄 Checkout Repository
        uses: actions/checkout@v4
        
      - name: 🐍 Setup Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
          
      - name: 📦 Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          pip install pytest-xdist pytest-benchmark pytest-mock
          
      - name: 🧪 Run Unit Tests
        run: |
          echo "🧪 Running unit tests with Python ${{ matrix.python-version }}..."
          
          # Run tests with coverage and parallel execution
          pytest tests/unit/ \
            --cov=ai_interlinq \
            --cov-report=xml \
            --cov-report=term-missing \
            --cov-report=html \
            --json-report \
            --json-report-file=unit_test_results.json \
            --benchmark-json=benchmark_results.json \
            -v \
            --tb=short \
            $([ "${{ env.PARALLEL_EXECUTION }}" = "true" ] && echo "-n auto" || echo "") \
            || true
          
      - name: 📊 Process Unit Test Results
        run: |
          python -c "
          import json
          import xml.etree.ElementTree as ET
          from datetime import datetime
          
          # Load test results
          try:
              with open('unit_test_results.json', 'r') as f:
                  test_data = json.load(f)
          except FileNotFoundError:
              test_data = {'summary': {'total': 0, 'passed': 0, 'failed': 0}}
          
          # Load coverage data
          coverage_percent = 0
          try:
              tree = ET.parse('coverage.xml')
              root = tree.getroot()
              coverage_percent = float(root.get('line-rate', 0)) * 100
          except (FileNotFoundError, ET.ParseError):
              pass
          
          # Create unit test summary
          unit_summary = {
              'timestamp': datetime.utcnow().isoformat(),
              'python_version': '${{ matrix.python-version }}',
              'test_type': 'unit',
              'law_compliance': 'LAW-001',
              'results': {
                  'total_tests': test_data.get('summary', {}).get('total', 0),
                  'passed': test_data.get('summary', {}).get('passed', 0),
                  'failed': test_data.get('summary', {}).get('failed', 0),
                  'skipped': test_data.get('summary', {}).get('skipped', 0),
                  'coverage_percent': round(coverage_percent, 2),
                  'coverage_threshold': ${{ env.COVERAGE_THRESHOLD }},
                  'meets_threshold': coverage_percent >= ${{ env.COVERAGE_THRESHOLD }}
              }
          }
          
          with open('unit_test_summary.json', 'w') as f:
              json.dump(unit_summary, f, indent=2)
          
          print(f'📊 Unit Tests Summary:')
          print(f'   Total: {unit_summary[\"results\"][\"total_tests\"]}')
          print(f'   Passed: {unit_summary[\"results\"][\"passed\"]}')
          print(f'   Failed: {unit_summary[\"results\"][\"failed\"]}')
          print(f'   Coverage: {unit_summary[\"results\"][\"coverage_percent\"]}%')
          "
          
      - name: 📋 Upload Unit Test Results
        uses: actions/upload-artifact@v3
        with:
          name: unit-test-results-py${{ matrix.python-version }}-${{ github.run_number }}
          path: |
            unit_test_results.json
            unit_test_summary.json
            coverage.xml
            htmlcov/
            benchmark_results.json
          retention-days: 30

  integration-tests:
    runs-on: ubuntu-latest
    if: contains(github.event.inputs.test_suite, 'integration') || contains(github.event.inputs.test_suite, 'all')
    services:
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
      - name: 🔄 Checkout Repository
        uses: actions/checkout@v4
        
      - name: 🐍 Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: 📦 Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          pip install pytest-asyncio pytest-timeout
          
      - name: 🌐 Setup Test Environment
        run: |
          echo "🌐 Setting up integration test environment..."
          
          # Set environment variables for integration tests
          echo "REDIS_URL=redis://localhost:6379" >> $GITHUB_ENV
          echo "TEST_ENVIRONMENT=integration" >> $GITHUB_ENV
          echo "LAW_001_COMPLIANCE=true" >> $GITHUB_ENV
          
      - name: 🔗 Run Integration Tests
        timeout-minutes: 30
        run: |
          echo "🔗 Running integration tests..."
          
          pytest tests/integration/ \
            --cov=ai_interlinq \
            --cov-append \
            --cov-report=xml \
            --cov-report=term-missing \
            --json-report \
            --json-report-file=integration_test_results.json \
            -v \
            --tb=short \
            --timeout=300 \
            || true
          
      - name: 📊 Process Integration Test Results
        run: |
          python -c "
          import json
          from datetime import datetime
          
          try:
              with open('integration_test_results.json', 'r') as f:
                  test_data = json.load(f)
          except FileNotFoundError:
              test_data = {'summary': {'total': 0, 'passed': 0, 'failed': 0}}
          
          integration_summary = {
              'timestamp': datetime.utcnow().isoformat(),
              'test_type': 'integration',
              'law_compliance': 'LAW-001',
              'environment': {
                  'redis_available': True,  # Assuming service is healthy
                  'test_timeout': 300
              },
              'results': {
                  'total_tests': test_data.get('summary', {}).get('total', 0),
                  'passed': test_data.get('summary', {}).get('passed', 0),
                  'failed': test_data.get('summary', {}).get('failed', 0),
                  'skipped': test_data.get('summary', {}).get('skipped', 0)
              }
          }
          
          with open('integration_test_summary.json', 'w') as f:
              json.dump(integration_summary, f, indent=2)
          
          print(f'📊 Integration Tests Summary:')
          print(f'   Total: {integration_summary[\"results\"][\"total_tests\"]}')
          print(f'   Passed: {integration_summary[\"results\"][\"passed\"]}')
          print(f'   Failed: {integration_summary[\"results\"][\"failed\"]}')
          "
          
      - name: 📋 Upload Integration Test Results
        uses: actions/upload-artifact@v3
        with:
          name: integration-test-results-${{ github.run_number }}
          path: |
            integration_test_results.json
            integration_test_summary.json
            coverage.xml
          retention-days: 30

  performance-tests:
    runs-on: ubuntu-latest
    if: contains(github.event.inputs.test_suite, 'performance') || contains(github.event.inputs.test_suite, 'all')
    
    steps:
      - name: 🔄 Checkout Repository
        uses: actions/checkout@v4
        
      - name: 🐍 Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: 📦 Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          pip install pytest-benchmark memory-profiler
          
      - name: ⚡ Run Performance Tests
        timeout-minutes: 45
        run: |
          echo "⚡ Running performance tests and benchmarks..."
          
          # Run existing benchmark suite
          python -c "
          import json
          import time
          import psutil
          import os
          from datetime import datetime
          from pathlib import Path
          
          def run_performance_benchmarks():
              results = {
                  'timestamp': datetime.utcnow().isoformat(),
                  'test_type': 'performance',
                  'law_compliance': 'LAW-001',
                  'system_info': {
                      'cpu_count': psutil.cpu_count(),
                      'memory_total': psutil.virtual_memory().total,
                      'python_version': '${{ env.PYTHON_VERSION }}'
                  },
                  'benchmarks': []
              }
              
              # Token management performance
              try:
                  from ai_interlinq import TokenManager
                  
                  start_time = time.perf_counter()
                  manager = TokenManager()
                  
                  # Benchmark token generation
                  tokens = []
                  for i in range(1000):
                      token = manager.generate_token(f'session_{i}')
                      tokens.append(token)
                  
                  generation_time = time.perf_counter() - start_time
                  
                  # Benchmark token validation
                  start_time = time.perf_counter()
                  for token in tokens[:100]:  # Validate first 100
                      try:
                          manager.validate_token(token)
                      except Exception:
                          pass
                  validation_time = time.perf_counter() - start_time
                  
                  results['benchmarks'].append({
                      'name': 'token_management',
                      'generation_time': generation_time,
                      'validation_time': validation_time,
                      'tokens_per_second': 1000 / generation_time,
                      'validations_per_second': 100 / validation_time
                  })
                  
              except Exception as e:
                  print(f'Token management benchmark failed: {e}')
              
              # Encryption performance
              try:
                  from ai_interlinq import EncryptionHandler
                  
                  encryption = EncryptionHandler()
                  test_data = 'x' * 10000  # 10KB test data
                  
                  start_time = time.perf_counter()
                  encrypted_results = []
                  for i in range(100):
                      success, encrypted = encryption.encrypt_message(test_data)
                      if success:
                          encrypted_results.append(encrypted)
                  encryption_time = time.perf_counter() - start_time
                  
                  # Benchmark decryption
                  start_time = time.perf_counter()
                  for encrypted in encrypted_results[:50]:
                      encryption.decrypt_message(encrypted)
                  decryption_time = time.perf_counter() - start_time
                  
                  results['benchmarks'].append({
                      'name': 'encryption',
                      'encryption_time': encryption_time,
                      'decryption_time': decryption_time,
                      'encryptions_per_second': 100 / encryption_time,
                      'decryptions_per_second': 50 / decryption_time,
                      'data_size': len(test_data)
                  })
                  
              except Exception as e:
                  print(f'Encryption benchmark failed: {e}')
              
              # Memory usage test
              try:
                  from ai_interlinq import MemorySystem
                  
                  process = psutil.Process()
                  initial_memory = process.memory_info().rss
                  
                  memory_system = MemorySystem()
                  
                  # Create many snapshots
                  for i in range(1000):
                      memory_system.create_snapshot({
                          'test_data': f'data_{i}',
                          'index': i,
                          'large_data': 'x' * 1000
                      }, [f'tag_{i % 10}'])
                  
                  final_memory = process.memory_info().rss
                  memory_increase = final_memory - initial_memory
                  
                  results['benchmarks'].append({
                      'name': 'memory_system',
                      'initial_memory_mb': initial_memory / 1024 / 1024,
                      'final_memory_mb': final_memory / 1024 / 1024,
                      'memory_increase_mb': memory_increase / 1024 / 1024,
                      'snapshots_created': 1000
                  })
                  
              except Exception as e:
                  print(f'Memory system benchmark failed: {e}')
              
              return results
          
          # Run benchmarks
          benchmark_results = run_performance_benchmarks()
          
          with open('performance_test_results.json', 'w') as f:
              json.dump(benchmark_results, f, indent=2)
          
          print('⚡ Performance benchmarks completed')
          for benchmark in benchmark_results['benchmarks']:
              print(f\"   {benchmark['name']}: ✅\")
          "
          
          # Run pytest-benchmark if available
          if [ -d "tests/performance" ]; then
            pytest tests/performance/ \
              --benchmark-json=pytest_benchmark_results.json \
              --benchmark-only \
              --benchmark-verbose \
              || true
          fi
          
      - name: 📊 Analyze Performance Results
        run: |
          python -c "
          import json
          from datetime import datetime
          
          try:
              with open('performance_test_results.json', 'r') as f:
                  results = json.load(f)
          except FileNotFoundError:
              results = {'benchmarks': []}
          
          # Analyze performance thresholds
          performance_analysis = {
              'timestamp': datetime.utcnow().isoformat(),
              'law_compliance': 'LAW-001',
              'analysis': {
                  'overall_status': 'unknown',
                  'issues': [],
                  'recommendations': []
              }
          }
          
          # Check performance against thresholds
          for benchmark in results.get('benchmarks', []):
              if benchmark['name'] == 'token_management':
                  if benchmark.get('tokens_per_second', 0) < 1000:
                      performance_analysis['analysis']['issues'].append({
                          'benchmark': 'token_management',
                          'issue': 'Token generation performance below threshold',
                          'threshold': 1000,
                          'actual': benchmark.get('tokens_per_second', 0)
                      })
              
              elif benchmark['name'] == 'encryption':
                  if benchmark.get('encryptions_per_second', 0) < 100:
                      performance_analysis['analysis']['issues'].append({
                          'benchmark': 'encryption',
                          'issue': 'Encryption performance below threshold',
                          'threshold': 100,
                          'actual': benchmark.get('encryptions_per_second', 0)
                      })
              
              elif benchmark['name'] == 'memory_system':
                  if benchmark.get('memory_increase_mb', 0) > 100:
                      performance_analysis['analysis']['issues'].append({
                          'benchmark': 'memory_system',
                          'issue': 'Memory usage increase too high',
                          'threshold': 100,
                          'actual': benchmark.get('memory_increase_mb', 0)
                      })
          
          # Overall status
          if not performance_analysis['analysis']['issues']:
              performance_analysis['analysis']['overall_status'] = 'passed'
          elif len(performance_analysis['analysis']['issues']) <= 2:
              performance_analysis['analysis']['overall_status'] = 'warning'
          else:
              performance_analysis['analysis']['overall_status'] = 'failed'
          
          with open('performance_analysis.json', 'w') as f:
              json.dump(performance_analysis, f, indent=2)
          
          print(f'📊 Performance Analysis:')
          print(f'   Status: {performance_analysis[\"analysis\"][\"overall_status\"].upper()}')
          print(f'   Issues: {len(performance_analysis[\"analysis\"][\"issues\"])}')
          "
          
      - name: 📋 Upload Performance Results
        uses: actions/upload-artifact@v3
        with:
          name: performance-test-results-${{ github.run_number }}
          path: |
            performance_test_results.json
            performance_analysis.json
            pytest_benchmark_results.json
          retention-days: 30

  law-compliance-tests:
    runs-on: ubuntu-latest
    if: contains(github.event.inputs.test_suite, 'law_compliance') || contains(github.event.inputs.test_suite, 'all')
    
    steps:
      - name: 🔄 Checkout Repository
        uses: actions/checkout@v4
        
      - name: 🐍 Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: 📦 Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          
      - name: 🏛️ Run LAW-001 Compliance Tests
        run: |
          echo "🏛️ Running comprehensive LAW-001 compliance tests..."
          
          # Run existing LAW-001 functional test
          python law001_functional_test.py --verbose --json-output > law001_test_results.json || true
          
          # Run LAW-001 verification
          python law001_verification.py --comprehensive > law001_verification_results.txt || true
          
          # Custom compliance tests
          python -c "
          import json
          import os
          import ast
          from datetime import datetime
          from pathlib import Path
          
          def check_law_compliance():
              results = {
                  'timestamp': datetime.utcnow().isoformat(),
                  'law_id': 'LAW-001',
                  'test_type': 'compliance_verification',
                  'checks': {},
                  'overall_compliance': True
              }
              
              # Check 1: Verify law.ai file exists and is valid
              try:
                  with open('law.ai', 'r') as f:
                      law_content = f.read()
                  
                  results['checks']['law_file'] = {
                      'exists': True,
                      'contains_law_001': 'LAW-001' in law_content,
                      'contains_learning_cycle': 'learning cycle' in law_content.lower(),
                      'contains_snapshot': 'snapshot' in law_content.lower()
                  }
                  
              except FileNotFoundError:
                  results['checks']['law_file'] = {
                      'exists': False,
                      'critical_error': True
                  }
                  results['overall_compliance'] = False
              
              # Check 2: Verify core learning cycle modules
              core_modules = [
                  'ai_interlinq/core/learning_cycle.py',
                  'ai_interlinq/core/snapshot_manager.py',
                  'ai_interlinq/core/pattern_detector.py',
                  'ai_interlinq/core/memory_loader.py'
              ]
              
              results['checks']['core_modules'] = {}
              for module_path in core_modules:
                  module_name = Path(module_path).stem
                  try:
                      with open(module_path, 'r') as f:
                          content = f.read()
                      
                      # Parse AST to check for required functions
                      tree = ast.parse(content)
                      functions = [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
                      classes = [node.name for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]
                      
                      results['checks']['core_modules'][module_name] = {
                          'exists': True,
                          'functions': len(functions),
                          'classes': len(classes),
                          'law_references': content.count('LAW-001') + content.count('law.ai')
                      }
                      
                  except FileNotFoundError:
                      results['checks']['core_modules'][module_name] = {
                          'exists': False,
                          'critical_error': True
                      }
                      results['overall_compliance'] = False
              
              # Check 3: Verify governance system
              governance_files = [
                  'governance/law_control.governance'
              ]
              
              results['checks']['governance'] = {}
              for gov_file in governance_files:
                  gov_name = Path(gov_file).stem
                  try:
                      with open(gov_file, 'r') as f:
                          content = f.read()
                      
                      results['checks']['governance'][gov_name] = {
                          'exists': True,
                          'has_voting_system': 'vote' in content.lower(),
                          'has_approval_system': 'approval' in content.lower()
                      }
                      
                  except FileNotFoundError:
                      results['checks']['governance'][gov_name] = {
                          'exists': False,
                          'warning': True
                      }
              
              # Check 4: Verify memory/snapshots directory structure
              memory_dirs = ['memory', 'memory/snapshots']
              results['checks']['memory_structure'] = {}
              
              for mem_dir in memory_dirs:
                  dir_name = mem_dir.replace('/', '_')
                  results['checks']['memory_structure'][dir_name] = {
                      'exists': Path(mem_dir).exists(),
                      'is_directory': Path(mem_dir).is_dir() if Path(mem_dir).exists() else False
                  }
                  
                  if Path(mem_dir).exists() and Path(mem_dir).is_dir():
                      files = list(Path(mem_dir).glob('*.json')) + list(Path(mem_dir).glob('*.ai'))
                      results['checks']['memory_structure'][dir_name]['file_count'] = len(files)
              
              # Check 5: Test core functionality
              try:
                  from ai_interlinq.core import learning_cycle, snapshot_manager
                  
                  # Test snapshot creation
                  snapshot_mgr = snapshot_manager.SnapshotManager()
                  test_snapshot = snapshot_mgr.create_snapshot({
                      'test': 'law_compliance_check',
                      'timestamp': datetime.utcnow().isoformat()
                  })
                  
                  results['checks']['functional_tests'] = {
                      'snapshot_creation': test_snapshot is not None,
                      'learning_cycle_import': True,
                      'core_modules_functional': True
                  }
                  
              except Exception as e:
                  results['checks']['functional_tests'] = {
                      'error': str(e),
                      'core_modules_functional': False
                  }
                  results['overall_compliance'] = False
              
              return results
          
          # Run compliance checks
          compliance_results = check_law_compliance()
          
          with open('law_compliance_test_results.json', 'w') as f:
              json.dump(compliance_results, f, indent=2)
          
          print('🏛️ LAW-001 Compliance Test Results:')
          print(f'   Overall Compliance: {\"✅ PASSED\" if compliance_results[\"overall_compliance\"] else \"❌ FAILED\"}')
          
          for check_category, checks in compliance_results['checks'].items():
              print(f'   {check_category.title()}:')
              if isinstance(checks, dict):
                  for check_name, result in checks.items():
                      if isinstance(result, dict):
                          status = '✅' if result.get('exists', result.get('core_modules_functional', True)) else '❌'
                          print(f'     {check_name}: {status}')
          "
          
      - name: 📋 Upload LAW Compliance Results
        uses: actions/upload-artifact@v3
        with:
          name: law-compliance-results-${{ github.run_number }}
          path: |
            law001_test_results.json
            law001_verification_results.txt
            law_compliance_test_results.json
          retention-days: 30

  test-summary:
    needs: [unit-tests, integration-tests, performance-tests, law-compliance-tests]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: 📥 Download All Test Results
        uses: actions/download-artifact@v3
        
      - name: 📊 Generate Comprehensive Test Summary
        run: |
          echo "📊 Generating comprehensive test summary..."
          
          python -c "
          import json
          import os
          from datetime import datetime
          from pathlib import Path
          
          def collect_test_results():
              summary = {
                  'timestamp': datetime.utcnow().isoformat(),
                  'law_compliance': 'LAW-001',
                  'test_suite': '${{ env.TEST_SUITE }}',
                  'coverage_threshold': ${{ env.COVERAGE_THRESHOLD }},
                  'overall_status': 'unknown',
                  'results': {},
                  'statistics': {
                      'total_tests': 0,
                      'total_passed': 0,
                      'total_failed': 0,
                      'total_skipped': 0,
                      'coverage_percent': 0,
                      'performance_status': 'unknown',
                      'compliance_status': 'unknown'
                  }
              }
              
              # Collect results from all artifact directories
              for artifact_dir in Path('.').glob('*/'):
                  if 'test-results' in artifact_dir.name:
                      # Find summary files
                      for summary_file in artifact_dir.glob('*_summary.json'):
                          try:
                              with open(summary_file, 'r') as f:
                                  data = json.load(f)
                              
                              test_type = data.get('test_type', 'unknown')
                              summary['results'][test_type] = data
                              
                              # Aggregate statistics
                              if 'results' in data:
                                  results = data['results']
                                  summary['statistics']['total_tests'] += results.get('total_tests', 0)
                                  summary['statistics']['total_passed'] += results.get('passed', 0)
                                  summary['statistics']['total_failed'] += results.get('failed', 0)
                                  summary['statistics']['total_skipped'] += results.get('skipped', 0)
                                  
                                  # Coverage (use highest coverage found)
                                  if results.get('coverage_percent', 0) > summary['statistics']['coverage_percent']:
                                      summary['statistics']['coverage_percent'] = results.get('coverage_percent', 0)
                              
                          except (json.JSONDecodeError, FileNotFoundError) as e:
                              print(f'Error reading {summary_file}: {e}')
              
              # Collect performance results
              for perf_file in Path('.').glob('*/performance_analysis.json'):
                  try:
                      with open(perf_file, 'r') as f:
                          perf_data = json.load(f)
                      summary['results']['performance'] = perf_data
                      summary['statistics']['performance_status'] = perf_data.get('analysis', {}).get('overall_status', 'unknown')
                  except (json.JSONDecodeError, FileNotFoundError):
                      pass
              
              # Collect LAW compliance results
              for law_file in Path('.').glob('*/law_compliance_test_results.json'):
                  try:
                      with open(law_file, 'r') as f:
                          law_data = json.load(f)
                      summary['results']['law_compliance'] = law_data
                      summary['statistics']['compliance_status'] = 'passed' if law_data.get('overall_compliance', False) else 'failed'
                  except (json.JSONDecodeError, FileNotFoundError):
                      pass
              
              # Determine overall status
              failed_tests = summary['statistics']['total_failed']
              coverage_ok = summary['statistics']['coverage_percent'] >= summary['coverage_threshold']
              performance_ok = summary['statistics']['performance_status'] in ['passed', 'warning']
              compliance_ok = summary['statistics']['compliance_status'] == 'passed'
              
              if failed_tests == 0 and coverage_ok and performance_ok and compliance_ok:
                  summary['overall_status'] = 'passed'
              elif failed_tests > 0 or not compliance_ok:
                  summary['overall_status'] = 'failed'
              else:
                  summary['overall_status'] = 'warning'
              
              return summary
          
          # Generate summary
          test_summary = collect_test_results()
          
          with open('comprehensive_test_summary.json', 'w') as f:
              json.dump(test_summary, f, indent=2)
          
          # Display summary
          print('📊 Comprehensive Test Summary:')
          print(f'   Overall Status: {test_summary[\"overall_status\"].upper()}')
          print(f'   Total Tests: {test_summary[\"statistics\"][\"total_tests\"]}')
          print(f'   Passed: {test_summary[\"statistics\"][\"total_passed\"]}')
          print(f'   Failed: {test_summary[\"statistics\"][\"total_failed\"]}')
          print(f'   Coverage: {test_summary[\"statistics\"][\"coverage_percent\"]}%')
          print(f'   Performance: {test_summary[\"statistics\"][\"performance_status\"].upper()}')
          print(f'   LAW-001 Compliance: {test_summary[\"statistics\"][\"compliance_status\"].upper()}')
          "
          
      - name: 📸 Create Testing Snapshot
        run: |
          echo "📸 Creating LAW-001 compliant testing snapshot..."
          
          python -c "
          import json
          import os
          from datetime import datetime
          
          # Load test summary
          try:
              with open('comprehensive_test_summary.json', 'r') as f:
                  test_data = json.load(f)
          except FileNotFoundError:
              test_data = {'overall_status': 'unknown'}
          
          # Create LAW-001 compliant snapshot
          snapshot = {
              'law_id': 'LAW-001',
              'timestamp': datetime.utcnow().isoformat(),
              'context': 'comprehensive_testing_pipeline',
              'input': {
                  'trigger': '${{ github.event_name }}',
                  'test_suite': '${{ env.TEST_SUITE }}',
                  'coverage_threshold': ${{ env.COVERAGE_THRESHOLD }},
                  'parallel_execution': '${{ env.PARALLEL_EXECUTION }}'
              },
              'action': 'comprehensive_testing_with_quality_gates',
              'applied_law': 'LAW-001',
              'reaction': 'testing_completed_with_compliance_verification',
              'output': {
                  'test_results': test_data,
                  'quality_gates': {
                      'coverage_gate': test_data.get('statistics', {}).get('coverage_percent', 0) >= ${{ env.COVERAGE_THRESHOLD }},
                      'compliance_gate': test_data.get('statistics', {}).get('compliance_status') == 'passed',
                      'overall_gate': test_data.get('overall_status') in ['passed', 'warning']
                  }
              },
              'ai_signature': 'mupoese_ai_testing_pipeline_v1.1.0',
              'compliance_verified': True
          }
          
          # Save snapshot
          os.makedirs('memory/snapshots', exist_ok=True)
          timestamp_str = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
          snapshot_file = f'memory/snapshots/testing_pipeline_{timestamp_str}.json'
          
          with open(snapshot_file, 'w') as f:
              json.dump(snapshot, f, indent=2)
          
          print(f'📸 Testing snapshot created: {snapshot_file}')
          "
          
      - name: 📋 Upload Final Test Summary
        uses: actions/upload-artifact@v3
        with:
          name: comprehensive-test-summary-${{ github.run_number }}
          path: |
            comprehensive_test_summary.json
            memory/snapshots/
          retention-days: 30
          
      - name: 🚦 Set Quality Gates
        run: |
          echo "🚦 Evaluating comprehensive testing quality gates..."
          
          python -c "
          import json
          import os
          
          try:
              with open('comprehensive_test_summary.json', 'r') as f:
                  summary = json.load(f)
          except FileNotFoundError:
              print('❌ No test summary found')
              exit(1)
          
          overall_status = summary.get('overall_status', 'unknown')
          stats = summary.get('statistics', {})
          
          print(f'Overall Status: {overall_status.upper()}')
          
          # Set GitHub outputs for downstream workflows
          print(f'::set-output name=overall-status::{overall_status}')
          print(f'::set-output name=total-tests::{stats.get(\"total_tests\", 0)}')
          print(f'::set-output name=coverage::{stats.get(\"coverage_percent\", 0)}')
          print(f'::set-output name=compliance::{stats.get(\"compliance_status\", \"unknown\")}')
          
          if overall_status == 'failed':
              print('❌ Quality gates failed - blocking deployment')
              exit(1)
          elif overall_status == 'warning':
              print('⚠️ Quality gates passed with warnings')
          else:
              print('✅ All quality gates passed')
          "