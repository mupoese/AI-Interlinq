# .github/workflows/ci.yml
name: CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, "3.10", "3.11"]

    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-dev.txt
    
    - name: Lint with flake8
      run: |
        flake8 ai_interlinq --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 ai_interlinq --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    
    - name: Check code formatting with black
      run: |
        black --check ai_interlinq/
    
    - name: Sort imports with isort
      run: |
        isort --check-only ai_interlinq/
    
    - name: Type check with mypy
      run: |
        mypy ai_interlinq/ --ignore-missing-imports
    
    - name: Test with pytest
      run: |
        pytest tests/ --cov=ai_interlinq --cov-report=xml --cov-report=term-missing
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  security:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install bandit safety
    
    - name: Security scan with bandit
      run: |
        bandit -r ai_interlinq/
    
    - name: Check dependencies with safety
      run: |
        safety check

---

# .github/workflows/release.yml
name: Release

on:
  push:
    tags:
      - 'v*'

jobs:
  build-and-publish:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build twine
    
    - name: Build package
      run: |
        python -m build
    
    - name: Check package
      run: |
        twine check dist/*
    
    - name: Publish to PyPI
      env:
        TWINE_USERNAME: __token__
        TWINE_PASSWORD: ${{ secrets.PYPI_API_TOKEN }}
      run: |
        twine upload dist/*
    
    - name: Create GitHub Release
      uses: actions/create-release@v1
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      with:
        tag_name: ${{ github.ref }}
        release_name: Release ${{ github.ref }}
        draft: false
        prerelease: false

---

# ai_interlinq/plugins/metrics.py
"""Metrics collection plugin for AI-Interlinq."""

import time
import threading
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from collections import defaultdict, deque
from enum import Enum

from ..utils.logging import get_logger


class MetricType(Enum):
    """Types of metrics."""
    COUNTER = "counter"
    GAUGE = "gauge"
    HISTOGRAM = "histogram"
    TIMER = "timer"


@dataclass
class Metric:
    """A single metric data point."""
    name: str
    value: float
    timestamp: float
    tags: Dict[str, str] = field(default_factory=dict)
    metric_type: MetricType = MetricType.GAUGE


class MetricsCollector:
    """Collects and aggregates metrics for AI-Interlinq."""
    
    def __init__(self, max_samples: int = 10000):
        self.max_samples = max_samples
        self.logger = get_logger("metrics_collector")
        
        # Thread-safe storage
        self._lock = threading.RLock()
        self._metrics: Dict[str, deque] = defaultdict(lambda: deque(maxlen=max_samples))
        self._counters: Dict[str, float] = defaultdict(float)
        self._gauges: Dict[str, float] = defaultdict(float)
        
        # Histogram buckets
        self._histogram_buckets = [0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
        self._histograms: Dict[str, Dict[float, int]] = defaultdict(lambda: defaultdict(int))
    
    def increment_counter(self, name: str, value: float = 1.0, tags: Optional[Dict[str, str]] = None) -> None:
        """Increment a counter metric."""
        with self._lock:
            metric_key = self._create_metric_key(name, tags)
            self._counters[metric_key] += value
            
            metric = Metric(
                name=name,
                value=value,
                timestamp=time.time(),
                tags=tags or {},
                metric_type=MetricType.COUNTER
            )
            self._metrics[metric_key].append(metric)
    
    def set_gauge(self, name: str, value: float, tags: Optional[Dict[str, str]] = None) -> None:
        """Set a gauge metric value."""
        with self._lock:
            metric_key = self._create_metric_key(name, tags)
            self._gauges[metric_key] = value
            
            metric = Metric(
                name=name,
                value=value,
                timestamp=time.time(),
                tags=tags or {},
                metric_type=MetricType.GAUGE
            )
            self._metrics[metric_key].append(metric)
    
    def record_histogram(self, name: str, value: float, tags: Optional[Dict[str, str]] = None) -> None:
        """Record a value in a histogram."""
        with self._lock:
            metric_key = self._create_metric_key(name, tags)
            
            # Find appropriate bucket
            for bucket in self._histogram_buckets:
                if value <= bucket:
                    self._histograms[metric_key][bucket] += 1
            
            # Also record in +Inf bucket
            self._histograms[metric_key][float('inf')] += 1
            
            metric = Metric(
                name=name,
                value=value,
                timestamp=time.time(),
                tags=tags or {},
                metric_type=MetricType.HISTOGRAM
            )
            self._metrics[metric_key].append(metric)
    
    def start_timer(self, name: str, tags: Optional[Dict[str, str]] = None) -> str:
        """Start a timer and return timer ID."""
        timer_id = f"{name}_{id(threading.current_thread())}_{time.time()}"
        
        with self._lock:
            if not hasattr(self, '_active_timers'):
                self._active_timers = {}
            
            self._active_timers[timer_id] = {
                'name': name,
                'start_time': time.time(),
                'tags': tags or {}
            }
        
        return timer_id
    
    def stop_timer(self, timer_id: str) -> Optional[float]:
        """Stop a timer and record the duration."""
        with self._lock:
            if not hasattr(self, '_active_timers') or timer_id not in self._active_timers:
                return None
            
            timer_info = self._active_timers.pop(timer_id)
            duration = time.time() - timer_info['start_time']
            
            # Record as histogram
            self.record_histogram(f"{timer_info['name']}_duration", duration, timer_info['tags'])
            
            return duration
    
    def get_counter_value(self, name: str, tags: Optional[Dict[str, str]] = None) -> float:
        """Get current counter value."""
        metric_key = self._create_metric_key(name, tags)
        with self._lock:
            return self._counters.get(metric_key, 0.0)
    
    def get_gauge_value(self, name: str, tags: Optional[Dict[str, str]] = None) -> Optional[float]:
        """Get current gauge value."""
        metric_key = self._create_metric_key(name, tags)
        with self._lock:
            return self._gauges.get(metric_key)
    
    def get_histogram_buckets(self, name: str, tags: Optional[Dict[str, str]] = None) -> Dict[float, int]:
        """Get histogram bucket counts."""
        metric_key = self._create_metric_key(name, tags)
        with self._lock:
            return dict(self._histograms.get(metric_key, {}))
    
    def get_all_metrics(self) -> Dict[str, List[Dict]]:
        """Get all metrics in Prometheus-like format."""
        with self._lock:
            result = {}
            
            # Counters
            for key, value in self._counters.items():
                if key not in result:
                    result[key] = []
                result[key].append({
                    'type': 'counter',
                    'value': value,
                    'timestamp': time.time()
                })
            
            # Gauges
            for key, value in self._gauges.items():
                if key not in result:
                    result[key] = []
                result[key].append({
                    'type': 'gauge',
                    'value': value,
                    'timestamp': time.time()
                })
            
            # Histograms
            for key, buckets in self._histograms.items():
                if key not in result:
                    result[key] = []
                result[key].append({
                    'type': 'histogram',
                    'buckets': dict(buckets),
                    'timestamp': time.time()
                })
            
            return result
    
    def export_prometheus_format(self) -> str:
        """Export metrics in Prometheus format."""
        lines = []
        
        with self._lock:
            # Export counters
            for key, value in self._counters.items():
                name, tags_str = self._parse_metric_key(key)
                lines.append(f"# TYPE {name} counter")
                lines.append(f"{name}{tags_str} {value}")
            
            # Export gauges
            for key, value in self._gauges.items():
                name, tags_str = self._parse_metric_key(key)
                lines.append(f"# TYPE {name} gauge")
                lines.append(f"{name}{tags_str} {value}")
            
            # Export histograms
            for key, buckets in self._histograms.items():
                name, tags_str = self._parse_metric_key(key)
                lines.append(f"# TYPE {name} histogram")
                
                for bucket, count in sorted(buckets.items()):
                    if bucket == float('inf'):
                        bucket_str = '+Inf'
                    else:
                        bucket_str = str(bucket)
                    
                    bucket_tags = tags_str.rstrip('}') + f',le="{bucket_str}"}}' if tags_str else f'{{le="{bucket_str}"}}'
                    lines.append(f"{name}_bucket{bucket_tags} {count}")
        
        return '\n'.join(lines)
    
    def clear_metrics(self) -> None:
        """Clear all metrics."""
        with self._lock:
            self._metrics.clear()
            self._counters.clear()
            self._gauges.clear()
            self._histograms.clear()
            if hasattr(self, '_active_timers'):
                self._active_timers.clear()
    
    def get_metrics_summary(self) -> Dict[str, Any]:
        """Get a summary of all metrics."""
        with self._lock:
            return {
                'total_metrics': len(self._metrics),
                'counters': len(self._counters),
                'gauges': len(self._gauges),
                'histograms': len(self._histograms),
                'active_timers': len(getattr(self, '_active_timers', {})),
                'memory_usage_bytes': sum(len(deque_) for deque_ in self._metrics.values()) * 64  # Rough estimate
            }
    
    def _create_metric_key(self, name: str, tags: Optional[Dict[str, str]]) -> str:
        """Create a unique key for the metric."""
        if not tags:
            return name
        
        tag_parts = [f"{k}={v}" for k, v in sorted(tags.items())]
        return f"{name}{{{','.join(tag_parts)}}}"
    
    def _parse_metric_key(self, key: str) -> tuple[str, str]:
        """Parse metric key back into name and tags string."""
        if '{' not in key:
            return key, ''
        
        name, tags_part = key.split('{', 1)
        return name, '{' + tags_part


# tests/test_core/test_token_manager.py
"""Tests for TokenManager."""

import pytest
import time
import asyncio
from ai_interlinq.core.token_manager import TokenManager, TokenStatus


class TestTokenManager:
    
    def test_generate_token(self):
        """Test token generation."""
        manager = TokenManager(default_ttl=300)
        
        token = manager.generate_token("test_session")
        
        assert token is not None
        assert len(token) > 0
        assert isinstance(token, str)
    
    def test_validate_token(self):
        """Test token validation."""
        manager = TokenManager(default_ttl=300)
        
        token = manager.generate_token("test_session")
        is_valid, session_id = manager.validate_token(token)
        
        assert is_valid is True
        assert session_id == "test_session"
    
    def test_invalid_token(self):
        """Test validation of invalid token."""
        manager = TokenManager()
        
        is_valid, session_id = manager.validate_token("invalid_token")
        
        assert is_valid is False
        assert session_id is None
    
    def test_token_expiration(self):
        """Test token expiration."""
        manager = TokenManager(default_ttl=1)  # 1 second TTL
        
        token = manager.generate_token("test_session")
        
        # Token should be valid initially
        is_valid, _ = manager.validate_token(token)
        assert is_valid is True
        
        # Wait for expiration
        time.sleep(1.1)
        
        # Token should be invalid after expiration
        is_valid, _ = manager.validate_token(token)
        assert is_valid is False
    
    def test_revoke_token(self):
        """Test token revocation."""
        manager = TokenManager()
        
        token = manager.generate_token("test_session")
        
        # Token should be valid initially
        is_valid, _ = manager.validate_token(token)
        assert is_valid is True
        
        # Revoke token
        success = manager.revoke_token("test_session")
        assert success is True
        
        # Token should be invalid after revocation
        is_valid, _ = manager.validate_token(token)
        assert is_valid is False
    
    def test_cleanup_expired_tokens(self):
        """Test cleanup of expired tokens."""
        manager = TokenManager(default_ttl=1)
        
        # Generate tokens
        token1 = manager.generate_token("session1")
        token2 = manager.generate_token("session2")
        
        # Wait for expiration
        time.sleep(1.1)
        
        # Cleanup expired tokens
        cleaned_count = manager.cleanup_expired_tokens()
        
        assert cleaned_count == 2
        
        # Tokens should be invalid
        is_valid1, _ = manager.validate_token(token1)
        is_valid2, _ = manager.validate_token(token2)
        
        assert is_valid1 is False
        assert is_valid2 is False
    
    def test_get_token_info(self):
        """Test getting token information."""
        manager = TokenManager()
        
        token = manager.generate_token("test_session")
        token_info = manager.get_token_info("test_session")
        
        assert token_info is not None
        assert token_info["session_id"] == "test_session"
        assert token_info["status"] == TokenStatus.ACTIVE.value
        assert "created_at" in token_info
        assert "expires_at" in token_info


# tests/test_core/test_encryption.py
"""Tests for EncryptionHandler."""

import pytest
from ai_interlinq.core.encryption import EncryptionHandler


class TestEncryptionHandler:
    
    def test_encryption_decryption_roundtrip(self):
        """Test encryption/decryption roundtrip."""
        handler = EncryptionHandler("test_key_123")
        
        original_message = "This is a test message for encryption"
        
        # Encrypt
        success, encrypted = handler.encrypt_message(original_message)
        assert success is True
        assert encrypted != original_message
        
        # Decrypt
        success, decrypted = handler.decrypt_message(encrypted)
        assert success is True
        assert decrypted == original_message
    
    def test_encryption_without_key(self):
        """Test encryption without setting a key."""
        handler = EncryptionHandler()
        
        success, error = handler.encrypt_message("test message")
        assert success is False
        assert "No encryption key set" in error
    
    def test_decryption_without_key(self):
        """Test decryption without setting a key."""
        handler = EncryptionHandler()
        
        success, error = handler.decrypt_message("fake_encrypted_data")
        assert success is False
        assert "No encryption key set" in error
    
    def test_set_shared_key(self):
        """Test setting shared key after initialization."""
        handler = EncryptionHandler()
        handler.set_shared_key("new_test_key")
        
        message = "Test message"
        success, encrypted = handler.encrypt_message(message)
        assert success is True
        
        success, decrypted = handler.decrypt_message(encrypted)
        assert success is True
        assert decrypted == message
    
    def test_generate_shared_key(self):
        """Test shared key generation."""
        handler = EncryptionHandler()
        
        key = handler.generate_shared_key()
        assert key is not None
        assert len(key) > 0
        assert isinstance(key, str)
    
    def test_message_hash(self):
        """Test message hash generation and verification."""
        handler = EncryptionHandler()
        
        message = "Test message for hashing"
        hash1 = handler.generate_message_hash(message)
        hash2 = handler.generate_message_hash(message)
        
        # Same message should produce same hash
        assert hash1 == hash2
        
        # Hash verification should work
        is_valid = handler.verify_message_hash(message, hash1)
        assert is_valid is True
        
        # Different message should produce different hash
        different_hash = handler.generate_message_hash("Different message")
        assert hash1 != different_hash
    
    def test_create_secure_session_key(self):
        """Test secure session key creation."""
        handler = EncryptionHandler("master_key")
        
        session_key1 = handler.create_secure_session_key("session1")
        session_key2 = handler.create_secure_session_key("session2")
        
        # Different sessions should have different keys
        assert session_key1 != session_key2
        
        # Same session should produce same key
        session_key1_again = handler.create_secure_session_key("session1")
        assert session_key1 == session_key1_again


# examples/advanced_features.py
"""Advanced features demonstration for AI-Interlinq."""

import asyncio
import time
from ai_interlinq import TokenManager, EncryptionHandler, CommunicationProtocol, MessageHandler
from ai_interlinq.core.communication_protocol import MessageType, Priority
from ai_interlinq.transport.websocket import WebSocketTransport, TransportConfig
from ai_interlinq.core.connection_manager import ConnectionManager
from ai_interlinq.plugins.load_balancer import LoadBalancer, LoadBalancingStrategy
from ai_interlinq.plugins.rate_limiter import RateLimiter, RateLimitConfig
from ai_interlinq.plugins.metrics import MetricsCollector
from ai_interlinq.utils.performance import PerformanceMonitor


async def advanced_communication_demo():
    """Demonstrate advanced AI-Interlinq features."""
    
    print("🚀 AI-Interlinq Advanced Features Demo")
    print("=" * 50)
    
    # Setup components
    shared_key = "advanced_demo_key_2024"
    
    # Agent A setup
    token_manager_a = TokenManager(default_ttl=3600)
    encryption_a = EncryptionHandler(shared_key)
    protocol_a = CommunicationProtocol("advanced_agent_a")
    message_handler_a = MessageHandler("advanced_agent_a", token_manager_a, encryption_a)
    
    # Agent B setup
    token_manager_b = TokenManager(default_ttl=3600)
    encryption_b = EncryptionHandler(shared_key)
    protocol_b = CommunicationProtocol("advanced_agent_b")
    message_handler_b = MessageHandler("advanced_agent_b", token_manager_b, encryption_b)
    
    # Setup WebSocket transport
    config = TransportConfig(host="localhost", port=8080)
    transport_a = WebSocketTransport(config)
    transport_a.set_message_handler(message_handler_a.receive_message)
    
    # Setup connection manager
    conn_manager = ConnectionManager(transport_a, "advanced_agent_a")
    await conn_manager.start()
    
    # Setup load balancer
    load_balancer = LoadBalancer(LoadBalancingStrategy.WEIGHTED_RANDOM)
    load_balancer.add_backend("agent_b_1", "localhost:8081", weight=1.0)
    load_balancer.add_backend("agent_b_2", "localhost:8082", weight=2.0)
    load_balancer.add_backend("agent_b_3", "localhost:8083", weight=1.5)
    
    # Setup rate limiter
    rate_limiter = RateLimiter()
    rate_limiter.set_agent_rate_limit(
        "advanced_agent_a",
        RateLimitConfig(requests_per_second=10.0, burst_size=20)
    )
    
    # Setup metrics collector
    metrics = MetricsCollector()
    
    # Setup performance monitor
    perf_monitor = PerformanceMonitor()
    
    print("✅ All components initialized")
    
    # Demo 1: Load-balanced message sending
    print("\n📡 Demo 1: Load-balanced Message Sending")
    
    for i in range(5):
        backend = load_balancer.select_backend()
        if backend:
            print(f"   Selected backend: {backend.agent_id} (weight: {backend.weight})")
            
            # Simulate response time
            response_time = 0.1 + (i * 0.02)
            load_balancer.update_backend_stats(backend.agent_id, response_time, True)
            
            # Update metrics
            metrics.increment_counter("messages_sent", tags={"backend": backend.agent_id})
            metrics.record_histogram("response_time", response_time, tags={"backend": backend.agent_id})
    
    # Demo 2: Rate limiting
    print("\n⏱️  Demo 2: Rate Limiting")
    
    allowed_count = 0
    denied_count = 0
    
    for i in range(15):  # Try to send 15 requests (limit is 10/sec)
        is_allowed = await rate_limiter.check_rate_limit("advanced_agent_a")
        if is_allowed:
            allowed_count += 1
            print(f"   Request {i+1}: ✅ Allowed")
        else:
            denied_count += 1
            print(f"   Request {i+1}: ❌ Rate limited")
    
    print(f"   Summary: {allowed_count} allowed, {denied_count} denied")
    
    # Demo 3: Performance monitoring
    print("\n📊 Demo 3: Performance Monitoring")
    
    # Simulate some operations
    for i in range(10):
        timer_id = perf_monitor.start_timer("message_processing")
        
        # Simulate work
        await asyncio.sleep(0.01)
        
        duration = perf_monitor.end_timer(timer_id)
        print(f"   Operation {i+1}: {duration*1000:.2f}ms")
    
    # Get performance stats
    stats = perf_monitor.get_metric_stats("message_processing_duration")
    if stats:
        print(f"   Average: {stats['mean']*1000:.2f}ms")
        print(f"   Min: {stats['min']*1000:.2f}ms")
        print(f"   Max: {stats['max']*1000:.2f}ms")
    
    # Demo 4: Advanced message patterns
    print("\n🔄 Demo 4: Advanced Message Patterns")
    
    session_id = "advanced_demo_session"
    
    # Request-Response pattern
    print("   Testing request-response pattern...")
    
    async def handle_data_request(message):
        """Handle data request."""
        print(f"   📥 Received data request: {message.payload.data}")
        
        # Send response
        response = protocol_b.create_message(
            recipient_id=message.header.sender_id,
            message_type=MessageType.RESPONSE,
            command="data_response",
            data={
                "processed_data": f"Processed: {message.payload.data['query']}",
                "timestamp": time.time(),
                "original_message_id": message.header.message_id
            },
            session_id=message.header.session_id
        )
        
        await message_handler_b.send_message(response)
    
    message_handler_b.register_command_handler("data_request", handle_data_request)
    
    # Create and send request
    request = protocol_a.create_message(
        recipient_id="advanced_agent_b",
        message_type=MessageType.REQUEST,
        command="data_request",
        data={"query": "get_user_data", "user_id": "12345"},
        session_id=session_id,
        priority=Priority.HIGH
    )
    
    # Use request-response pattern
    response = await message_handler_a.send_request_and_wait_response(request, timeout=5.0)
    
    if response:
        print(f"   📤 Received response: {response.payload.data['processed_data']}")
    else:
        print("   ❌ Request timed out")
    
    # Demo 5: Metrics export
    print("\n📈 Demo 5: Metrics Export")
    
    # Add some more metrics
    metrics.set_gauge("active_connections", 5)
    metrics.set_gauge("memory_usage_mb", 128.5)
    metrics.increment_counter("total_requests", 100)
    
    # Export in Prometheus format
    prometheus_metrics = metrics.export_prometheus_format()
    print("   Prometheus format export:")
    print("   " + "\n   ".join(prometheus_metrics.split("\n")[:10]) + "...")
    
    # Get metrics summary
    summary = metrics.get_metrics_summary()
    print(f"   Metrics summary: {summary}")
    
    # Cleanup
    await conn_manager.stop()
    
    print("\n✅ Advanced features demo completed!")


async def distributed_setup_demo():
    """Demonstrate distributed AI agent setup."""
    
    print("\n🌐 Distributed Setup Demo")
    print("=" * 30)
    
    # This would typically involve multiple processes/containers
    # For demo purposes, we'll simulate the setup
    
    agents = ["ai_agent_1", "ai_agent_2", "ai_agent_3"]
    load_balancer = LoadBalancer(LoadBalancingStrategy.LEAST_CONNECTIONS)
    
    # Register agents with load balancer
    for i, agent_id in enumerate(agents):
        address = f"localhost:{8080 + i}"
        load_balancer.add_backend(agent_id, address)
        print(f"   Registered {agent_id} at {address}")
    
    # Simulate traffic distribution
    print("\n   Simulating traffic distribution:")
    
    for i in range(10):
        backend = load_balancer.select_backend()
        if backend:
            load_balancer.increment_connections(backend.agent_id)
            print(f"   Request {i+1} -> {backend.agent_id} (connections: {backend.active_connections})")
            
            # Simulate request completion
            await asyncio.sleep(0.1)
            load_balancer.decrement_connections(backend.agent_id)
    
    # Show final stats
    stats = load_balancer.get_backend_stats()
    print("\n   Final backend statistics:")
    for agent_id, stat in stats.items():
        print(f"   {agent_id}: {stat}")
    
    print("\n✅ Distributed setup demo completed!")


if __name__ == "__main__":
    print("🎯 Starting AI-Interlinq Advanced Demos...")
    
    # Run advanced communication demo
    asyncio.run(advanced_communication_demo())
    
    # Run distributed setup demo
    asyncio.run(distributed_setup_demo())
    
    print("\n🎉 All advanced demos completed successfully!")
